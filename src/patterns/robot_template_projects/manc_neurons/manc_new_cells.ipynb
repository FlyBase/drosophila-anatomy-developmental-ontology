{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573bc88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "get cell type info from manc \n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import neuprint\n",
    "\n",
    "# for connecting to neuPrint (add token)\n",
    "token = \"\"\n",
    "np_client = neuprint.Client('https://neuprint.janelia.org', dataset='manc:v1.2.1', token=token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b1d4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cell type info\n",
    "query = (\"MATCH (n:Neuron) RETURN DISTINCT n.systematicType, n.type, n.class, n.subclass, \"\n",
    "         \"n.somaNeuromere, n.hemilineage, n.birthtime, \"\n",
    "         \"n.target, n.origin, n.celltypePredictedNt, n.entryNerve, n.exitNerve, n.longTract, count(n) AS count\")\n",
    "np_results_raw = np_client.fetch_custom(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a428ff74",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_results = np_results_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfd499c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "np_results = np_results.rename(columns=(lambda x: x.removeprefix('n.')))\n",
    "np_results = np_results.rename(columns=({'class': 'cell_class'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3bc86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some preprocessing\n",
    "# drop rows where type is null or ends with xx instead of number\n",
    "np_results = np_results[np_results['type'].notnull()]\n",
    "np_results = np_results[~(np_results['type'].str.endswith('xx') | np_results['type'].str.endswith('XX'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a473e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep cell types that are in new_cell_FBbt_ids.tsv\n",
    "fbbt_mapping = pd.read_csv('new_cell_FBbt_ids.tsv', low_memory=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c333e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_results = np_results[np_results['type'].isin(fbbt_mapping['type'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48d63e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert null-like values to nulls\n",
    "np_results = np_results.replace({'<NA>': None, 'NA': None, 'TBD': None, 'none': None, 'None': None,\n",
    "                                 'unclear': None, 'unknown': None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33136298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# minor replacements\n",
    "np_results['cell_class'] = np_results['cell_class'].replace('Sensory TBD', 'sensory neuron')\n",
    "np_results['birthtime'] = np_results['birthtime'].replace('early secondary', 'secondary')\n",
    "np_results['type'] = np_results['type'].replace('oviDN', 'DNad001')\n",
    "np_results['origin'] = np_results['origin'].map(lambda x: x.replace('tct', 'Tct'), na_action='ignore')\n",
    "np_results = np_results.replace('AbNTBD', 'AbN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08756961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split target and origin on . if the . is not followed by a space\n",
    "# drop sides from target, origin\n",
    "def process_regions(x):\n",
    "    if not x:\n",
    "        return None\n",
    "    else:\n",
    "        x = x.strip('. ')\n",
    "        x = re.sub('\\. ', ' ', x)\n",
    "        x = re.sub('[LR]HS ', '', x)\n",
    "        return '|'.join(list(set([y.strip(',') for y in re.split('\\.|_', x) \n",
    "                                  if y.strip(',') not in ['L', 'R', 'RL', 'LR']])))\n",
    "np_results['target'] = np_results['target'].map(process_regions, na_action='ignore')\n",
    "np_results['origin'] = np_results['origin'].map(process_regions, na_action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32ef40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split nerves on space or _ and drop commas and L/R\n",
    "def process_nerves(x):\n",
    "    if not x:\n",
    "        return None\n",
    "    else:\n",
    "        return '|'.join(list(set([y.strip(',') for y in re.split(' |_', x) if y.strip(',') not in ['L', 'R']])))\n",
    "np_results['entryNerve'] = np_results['entryNerve'].map(process_nerves, na_action='ignore')\n",
    "np_results['exitNerve'] = np_results['exitNerve'].map(process_nerves, na_action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c22cb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate duplicate info rows and sum count\n",
    "np_results_grouped = np_results.groupby([\"type\"], dropna=False).agg({\n",
    "    \"cell_class\": set, \n",
    "    \"hemilineage\": set,\n",
    "    'systematicType': set,\n",
    "    \"subclass\": set, \n",
    "    'somaNeuromere': set, \n",
    "    \"birthtime\": set, \n",
    "    \"celltypePredictedNt\": set, \n",
    "    \"target\": set, \n",
    "    \"origin\": set, \n",
    "    \"entryNerve\": set, \n",
    "    \"exitNerve\": set, \n",
    "    \"longTract\": set, \n",
    "    'count': 'sum'\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e40d8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt to find common denominators for multiple value fields\n",
    "segment_prefixes = ['prothoracic ', 'mesothoracic ', 'metathoracic ', 'front ', 'middle ', 'hind ']\n",
    "segment_suffixes = ['T1', 'T2', 'T3', ' A1', ' A2', ' A3']\n",
    "\n",
    "abdominal_nerves = ['AbN', 'AbNT', 'AbN1', 'AbN2', 'AbN3', 'AbN4', 'AbN5']\n",
    "tectulums = ['Tct', 'LTct', 'HTct', 'IntTct', 'UTct', 'WTct']\n",
    "\n",
    "def map_multiple_to_terms_in_common(input_set):\n",
    "    \"\"\"\n",
    "    1. check for an entry of None (at least one individual has no info)\n",
    "    2. unpack lists from set of strings (giving list of sets)\n",
    "    3. consider 'multi' to match all and drop from set\n",
    "    4. check if an item appears in all sets and add to common_terms set\n",
    "    5. remove any common terms from the listed sets\n",
    "    6. remove segment specificity substrings (prefixes and suffixes above)\n",
    "    7. check if an item appears in all sets and add to common_terms set\n",
    "    8. check if all sets contain an abdominal nerve (if so add 'AbN')\n",
    "    9. check if all sets contain a type of tectulum (if so add 'Tct')\n",
    "    \"\"\"\n",
    "    if None in input_set:\n",
    "        return {None}\n",
    "    unpacked_set = [set(x.split('|')) for x in input_set if x]\n",
    "    if len(unpacked_set) > 1:\n",
    "        try:\n",
    "            unpacked_set.remove('multi')\n",
    "        except ValueError:\n",
    "            pass\n",
    "        if len(unpacked_set) == 1:\n",
    "            return unpacked_set[0]\n",
    "        \n",
    "        common_terms = set.intersection(*unpacked_set)\n",
    "        unpacked_set = [set([b for b in a if b not in common_terms]) for a in unpacked_set]\n",
    "        \n",
    "        neutralised_list_of_sets = [set([b.removeprefix(p).removesuffix(s) \n",
    "                                         for p in segment_prefixes for s in segment_suffixes for b in a]) \n",
    "                                    for a in unpacked_set]\n",
    "        \n",
    "        common_neutral_terms = set.intersection(*neutralised_list_of_sets)\n",
    "        common_terms.update(common_neutral_terms)\n",
    "        \n",
    "        if all(any(b in abdominal_nerves for b in a) for a in neutralised_list_of_sets):\n",
    "            common_terms.update(['AbN'])\n",
    "        \n",
    "        if all(any(b in tectulums for b in a) for a in neutralised_list_of_sets):\n",
    "            common_terms.update(['Tct'])\n",
    "        \n",
    "        if len(common_terms) > 0:\n",
    "            return common_terms\n",
    "        else:\n",
    "            return {None}\n",
    "        \n",
    "    elif len(unpacked_set) == 1:\n",
    "        return unpacked_set[0]\n",
    "    else:\n",
    "        return {None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb72fc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_process = ['target', 'origin', 'entryNerve', 'exitNerve', 'longTract']\n",
    "for col in cols_to_process:\n",
    "    np_results_grouped['common_' + col] = np_results_grouped[col].map(map_multiple_to_terms_in_common)\n",
    "\n",
    "np_results_grouped = np_results_grouped.drop(columns=cols_to_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4917e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix only some of a type classed as ascending\n",
    "def drop_ascending(set_of_classes):\n",
    "    if len(set_of_classes) > 1:\n",
    "        for t in ['sensory', 'afferent']:\n",
    "            if all(t in x for x in set_of_classes):\n",
    "                cell_class = t + ' neuron'\n",
    "            else:\n",
    "                cell_class = {None}\n",
    "        return cell_class\n",
    "    else:\n",
    "        return set_of_classes\n",
    "    \n",
    "np_results_grouped['cell_class'] = np_results_grouped['cell_class'].map(drop_ascending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcecc181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map {None} to empty set (don't want to lose None from sets len > 1)\n",
    "np_results_grouped_filtered = np_results_grouped.map(lambda x: x - {None} if ((type(x) is set) and len(x)==1) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5763566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_results_grouped_filtered.to_csv('typing_info.tsv', sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932803fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FBbt mapping files - probably don't need to update this\n",
    "\n",
    "# get unique values from two columns of lists\n",
    "entry = np_results_grouped['common_entryNerve'].explode().drop_duplicates()\n",
    "exit = np_results_grouped['common_exitNerve'].explode().drop_duplicates()\n",
    "nerves = pd.concat([exit, entry.rename({'common_entryNerve':'common_exitNerve'})]).drop_duplicates()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fcfa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_class = np_results_grouped['cell_class'].explode().drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27250dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_class.to_csv('class_FBbt_map2.tsv', sep='\\t', index=None) # change to do others too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60486bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

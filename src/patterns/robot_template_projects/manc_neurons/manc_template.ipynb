{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e263fb3",
   "metadata": {},
   "source": [
    "Notebook to make a robot template for the new manc cell types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7f27ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753657af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids for types\n",
    "new_cell_FBbt_ids = pd.read_csv('new_cell_FBbt_ids.tsv', sep='\\t', index_col='type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb24449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open type detail files\n",
    "typing_info = pd.read_csv('typing_info.tsv', sep='\\t', index_col='type', \n",
    "                          dtype={'defaultdict': 'object', 'count': 'int', 'type': 'str'})\n",
    "\n",
    "set_cols = typing_info.columns.drop('count')\n",
    "typing_info[set_cols] = typing_info[set_cols].map(ast.literal_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d98cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_FBbt_map = pd.read_csv('class_FBbt_map.tsv', sep='\\t', index_col='term')\n",
    "subclass_detail = pd.read_csv('subclass_detail.tsv', sep='\\t', index_col='term', na_filter=False, dtype=str)\n",
    "hemilineage_notch_FBbt_map = pd.read_csv('hemilineage_notch_FBbt_map.tsv', sep='\\t', index_col='term')\n",
    "hemilineage_nb_FBbt_map = pd.read_csv('hemilineage_nb_FBbt_map.tsv', sep='\\t', index_col='term')\n",
    "birthtime_FBbt_map = pd.read_csv('birthtime_FBbt_map.tsv', sep='\\t', index_col='term')\n",
    "nerve_FBbt_map = pd.read_csv('nerve_FBbt_map.tsv', sep='\\t', index_col='term')\n",
    "neuromere_FBbt_map = pd.read_csv('neuromere_FBbt_map.tsv', sep='\\t', index_col='term')\n",
    "region_FBbt_map = pd.read_csv('region_FBbt_map.tsv', sep='\\t', index_col='term')\n",
    "region_FBbt_map.loc['multi', 'FBbt_name'] = 'multiple regions'\n",
    "tract_FBbt_map = pd.read_csv('tract_FBbt_map.tsv', sep='\\t', index_col='term')\n",
    "nt_go_map = pd.read_csv('nt_go_map.tsv', sep='\\t', index_col='term')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774a9706",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_types = {'AN':'ascending neuron', 'DN':'descending neuron', \n",
    "               'EA': 'efferent ascending neuron', 'EN': 'efferent neuron',\n",
    "               'MN':'motor neuron', 'SN':'sensory neuron', \n",
    "               'SA':'sensory ascending neuron', 'IN':'intrinsic neuron'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429c4c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_lookup(term, mapping):\n",
    "    try:\n",
    "        FBbt_id = mapping.loc[term, 'FBbt_id']\n",
    "    except:\n",
    "        return None\n",
    "    if type(FBbt_id) == str:\n",
    "        return FBbt_id\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bd8848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_lookup(term, mapping):\n",
    "    try:\n",
    "        FBbt_label = mapping.loc[term, 'FBbt_name']\n",
    "    except:\n",
    "        return None\n",
    "    if type(FBbt_label) == str:\n",
    "        return FBbt_label.removeprefix('adult ')\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0df05bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_lister(names):\n",
    "    L = \"\"\n",
    "    names.sort()\n",
    "    if len(names) < 1:\n",
    "        return False\n",
    "    elif len(names) == 1:\n",
    "        return names[0]\n",
    "    elif len(names) > 1:\n",
    "        L = names[0]\n",
    "        if len(names) > 2:\n",
    "            for i in names[1:-1]:\n",
    "                L = L + \", \" + i\n",
    "        L = L + \" and \" + names[-1]\n",
    "        return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af738627",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "# template header\n",
    "template_head = OrderedDict([('ID', 'ID'), ('TYPE', 'TYPE'), ('Label', 'LABEL'), \n",
    "                             (\"obo_id\" , \"A oboInOwl:id\"), (\"obo_namespace\" , \"A oboInOwl:hasOBONamespace\"), \n",
    "                             ('Definition', 'A IAO:0000115'), \n",
    "                             ('Def_xrefs', '>A oboInOwl:hasDbXref SPLIT=|'), ('Comment', 'A rdfs:comment'),  \n",
    "                             ('Creator', 'AI dc:contributor'), \n",
    "                             ('Date', 'AT dc:date^^xsd:dateTime'), \n",
    "                             ('Soma', 'SC RO:0002100 some %'), ('Parents', 'SC % SPLIT=|'), \n",
    "                             ('Lineage','SC RO:0002202 some %'), ('Laterality', 'SC RO:0000053 some %'), \n",
    "                             ('Projection_bundles', 'SC RO:0002101 some % SPLIT=|'),\n",
    "                             ('Neurotransmitter', 'SC RO:0002215 some %'), \n",
    "                             ('Presynapses', 'SC RO:0013003 some % SPLIT=|'), \n",
    "                             ('Postsynapses', 'SC RO:0013002 some % SPLIT=|'), \n",
    "                             ('Sens_dend', 'SC RO:0013007 some % SPLIT=|')])\n",
    "template = pd.DataFrame.from_dict([template_head])\n",
    "#template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707cba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build template one row at a time for each new class\n",
    "refs = ' (Takemura et al., 2023; Marin et al., 2024).'\n",
    "for i in new_cell_FBbt_ids.index: # index is the type name (same as index for typing_info)\n",
    "    \n",
    "    # create a dictionary to hold information for this class\n",
    "    row = OrderedDict([])\n",
    "    for c in template.columns:\n",
    "        row[c] = \"\" # setting all as \"\" now to avoid awkward NaNs later\n",
    "\n",
    "    # populate row dictionary\n",
    "    \n",
    "    # annotation axioms\n",
    "    row['ID'] = new_cell_FBbt_ids['FBbt_id'][i]\n",
    "    row['obo_id'] = new_cell_FBbt_ids['FBbt_id'][i]\n",
    "    row['obo_namespace'] = 'fly_anatomy.ontology'\n",
    "    row['TYPE'] = 'owl:Class'\n",
    "    row['Label'] = f'adult {i} neuron'\n",
    "    # xrefs for Marin et al. (2024), Takemura et al. (2023), Cheong et al. (2024)\n",
    "    row['Def_xrefs'] = 'doi:10.1101/2023.06.05.543407|doi:10.1101/2023.06.05.543757|doi:10.7554/eLife.96084.1'\n",
    "    row['Comment'] = (\"Uncharacterized putative cell type from Marin et al. (2024), based on \"\n",
    "                      \"MANC v1.2.1 data (Takemura et al., 2023) from NeuPrint.\")\n",
    "                      \n",
    "    row['Creator'] = \"https://orcid.org/0000-0002-1373-1705\"\n",
    "    if row['ID'] in ['FBbt:20004627', 'FBbt:20004148']:\n",
    "        row['Date'] = \"2024-07-24T12:00:00Z\"\n",
    "    else:\n",
    "        row['Date'] = \"2024-05-10T12:00:00Z\"\n",
    "    \n",
    "    # establish lists for collecting multiple elements\n",
    "    Parents_list = []\n",
    "    projection_bundles = []\n",
    "    definition_components = []\n",
    "    \n",
    "    # process each column of the data to add logical axioms and build definition\n",
    "\n",
    "    # class\n",
    "    try:\n",
    "        [cell_class] = typing_info.loc[i, 'cell_class']\n",
    "    except ValueError:\n",
    "        cell_class = short_types.get(i[0:2], 'neuron')\n",
    "    Parents_list.append(cv_lookup(cell_class, class_FBbt_map))\n",
    "    if 'neuron' not in cell_class:\n",
    "        cell_class += ' neuron'\n",
    "    \n",
    "    \n",
    "    # info from subclass\n",
    "    type_update = ''\n",
    "    to_append = ''\n",
    "    \n",
    "    name_subclass = i[2:4]\n",
    "    try:\n",
    "        if subclass_detail.loc[name_subclass, 'parent']:\n",
    "            Parents_list.append(subclass_detail.loc[name_subclass, 'parent'])\n",
    "        type_update += subclass_detail.loc[name_subclass, 'type_update']\n",
    "        to_append += subclass_detail.loc[name_subclass, 'append']\n",
    "    except KeyError:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        [subclass] = typing_info.loc[i, 'subclass']\n",
    "        if subclass_detail.loc[subclass, 'parent']:\n",
    "            Parents_list.append(subclass_detail.loc[subclass, 'parent'])\n",
    "        row['Laterality'] = subclass_detail.loc[subclass, 'laterality']\n",
    "        type_update += ' ' + subclass_detail.loc[subclass, 'type_update']\n",
    "        to_append += ' ' + subclass_detail.loc[subclass, 'append']\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    first_sentence = ' '.join(['Adult', type_update,\n",
    "                                   cell_class, to_append]).replace('  ', ' ').strip(' ')\n",
    "    first_sentence = re.sub('[ ]+', ' ', first_sentence)\n",
    "    first_sentence = re.sub('sensory sensory', 'sensory', first_sentence)\n",
    "    \n",
    "    definition_components.append(first_sentence + refs)\n",
    "\n",
    "    # birth time and hemilineage\n",
    "    \n",
    "    try:\n",
    "        [birthtime] = typing_info.loc[i, 'birthtime']\n",
    "        Parents_list.append(cv_lookup(birthtime, birthtime_FBbt_map))\n",
    "        birth_lineage = f\"It is a {birthtime} neuron\"\n",
    "        try:\n",
    "            [hemilineage] = typing_info.loc[i, 'hemilineage']\n",
    "            Parents_list.append(cv_lookup(hemilineage, hemilineage_notch_FBbt_map))\n",
    "            row['Lineage'] = cv_lookup(hemilineage, hemilineage_nb_FBbt_map)\n",
    "            birth_lineage += f\" of the {hemilineage} hemilineage\"\n",
    "        except ValueError:\n",
    "            pass\n",
    "        definition_components.append(birth_lineage + refs)\n",
    "    except ValueError:\n",
    "        try:\n",
    "            [hemilineage] = typing_info.loc[i, 'hemilineage']\n",
    "            Parents_list.append(cv_lookup(hemilineage, hemilineage_notch_FBbt_map))\n",
    "            row['Lineage'] = cv_lookup(hemilineage, hemilineage_nb_FBbt_map)\n",
    "            definition_components.append(f\"It belongs to the {hemilineage} hemilineage\" + refs)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    # nerves and tracts\n",
    "    nerves = ''\n",
    "    try:\n",
    "        [entry_nerve] = typing_info.loc[i, 'common_entryNerve']\n",
    "        projection_bundles.append(cv_lookup(entry_nerve, nerve_FBbt_map))\n",
    "        nerves += f'It enters the VNC via the {label_lookup(entry_nerve, nerve_FBbt_map)}'\n",
    "        exit_nerve_join = ' and exits via the'\n",
    "    except ValueError:\n",
    "        exit_nerve_join = 'It exits the VNC via the'\n",
    "    if len(typing_info.loc[i, 'common_exitNerve']) > 0:\n",
    "        projection_bundles.extend([cv_lookup(n, nerve_FBbt_map) for n in typing_info.loc[i, 'common_exitNerve']])\n",
    "        exitnerves = list(set([label_lookup(l, nerve_FBbt_map) for l in typing_info.loc[i, 'common_exitNerve']]))\n",
    "        nerves += f\"{exit_nerve_join} {name_lister(exitnerves)}\"\n",
    "    if nerves:\n",
    "        definition_components.append(nerves + refs)\n",
    "    \n",
    "    if len(typing_info.loc[i, 'common_longTract']) > 0:\n",
    "        projection_bundles.extend([cv_lookup(n, tract_FBbt_map) for n in typing_info.loc[i, 'common_longTract']])\n",
    "        tracts = list(set([label_lookup(l, tract_FBbt_map) for l in typing_info.loc[i, 'common_longTract']]))\n",
    "        definition_components.append(f'Within the VNC it fasciculates with the {name_lister(tracts)}' + refs)\n",
    "    \n",
    "\n",
    "    # regions\n",
    "    synapses = ''\n",
    "    if len(typing_info.loc[i, 'common_origin']) > 0:\n",
    "        origin_ids = list(set([cv_lookup(l, region_FBbt_map) for l in typing_info.loc[i, 'common_origin']]))\n",
    "        if ('sensory' in cell_class):\n",
    "            row['Sens_dend'] = '|'.join([i for i in origin_ids if i])\n",
    "        else:\n",
    "            row['Postsynapses'] = '|'.join([i for i in origin_ids if i])\n",
    "        origin_names = list(set([label_lookup(l, region_FBbt_map) for l in typing_info.loc[i, 'common_origin']]))\n",
    "        if any(origin_names):\n",
    "            synapses += f\"It receives input in the {name_lister([i for i in origin_names if i])}\"\n",
    "            synapse_join = \" and\"\n",
    "        else:\n",
    "            synapse_join = \"It\"\n",
    "    else:\n",
    "        synapse_join = \"It\"\n",
    "        \n",
    "    if len(typing_info.loc[i, 'common_target']) > 0:\n",
    "        target_ids = list(set([cv_lookup(l, region_FBbt_map) for l in typing_info.loc[i, 'common_target']]))\n",
    "        row['Presynapses'] = '|'.join([i for i in target_ids if i])\n",
    "        target_names = list(set([label_lookup(l, region_FBbt_map) for l in typing_info.loc[i, 'common_target']]))\n",
    "        if any(target_names):\n",
    "            synapses += f\"{synapse_join} sends output to the {name_lister([i for i in target_names if i])}\"\n",
    "    if synapses:\n",
    "        synapses = re.sub('the multiple', 'multiple', synapses)\n",
    "        definition_components.append(synapses + refs)\n",
    "    \n",
    "    # nt\n",
    "    try:\n",
    "        [neurotransmitter] = typing_info.loc[i, 'celltypePredictedNt']\n",
    "        row['Neurotransmitter'] = cv_lookup(neurotransmitter, nt_go_map)\n",
    "        definition_components.append(f\"Its predicted neurotransmitter is {neurotransmitter} (Eckstein et al., 2024).\")\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    # cell number and somas\n",
    "    cell_soma = ''\n",
    "    cell_count = typing_info.loc[i, 'count']\n",
    "    if cell_count == 1:\n",
    "        cell_soma += \"There is approximately one of these cells per organism\"\n",
    "        soma_mod = ' with its soma in'\n",
    "    elif cell_count > 1:\n",
    "        cell_soma += f\"There are approximately {str(cell_count)} of these cells per organism\"\n",
    "        soma_mod = ' and their somas are found in'\n",
    "    else:\n",
    "        raise ValueError(\"Cell count must be >= 1\")\n",
    "    \n",
    "    try:\n",
    "        [somaNeuromere] = typing_info.loc[i, 'somaNeuromere']\n",
    "        row['Soma'] = cv_lookup(somaNeuromere, neuromere_FBbt_map)\n",
    "        cell_soma += f\"{soma_mod} the {label_lookup(somaNeuromere, neuromere_FBbt_map)}\"\n",
    "    except ValueError:\n",
    "        if len(typing_info.loc[i, 'somaNeuromere'])>1:\n",
    "            neuromeres = list(set([\n",
    "                label_lookup(l, neuromere_FBbt_map) for l in typing_info.loc[i, 'somaNeuromere']]))\n",
    "            cell_soma += f\"{soma_mod} the {name_lister(neuromeres)}\"\n",
    "        else:\n",
    "            pass\n",
    "    if cell_soma:\n",
    "        definition_components.append(cell_soma + refs)\n",
    "    \n",
    "    # add multi-part entries (drop any Nones from ID lists)\n",
    "    row['Parents'] = '|'.join([i for i in Parents_list if type(i)==str])\n",
    "    row['Projection_bundles'] = '|'.join([i for i in projection_bundles if type(i)==str])\n",
    "    row['Definition'] = ' '.join(definition_components)\n",
    "        \n",
    "    # turn into dataframe and join to template\n",
    "    row_df = pd.DataFrame.from_dict([row])\n",
    "    template = pd.concat([template, row_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0071fc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add rows to specify TYPE for other classes and OPs\n",
    "# this is a workaround for a robot bug - https://github.com/ontodev/robot/issues/1105\n",
    "\n",
    "# function to get curies\n",
    "def extract_uris(text):\n",
    "    return re.findall(r'\\b(?:FBbt|GO|PATO|RO):\\d+\\b', str(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d89d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "curies = []\n",
    "for column in template.columns:\n",
    "    if column != 'ID':\n",
    "        new_uris = template[column].apply(extract_uris)\n",
    "        # Filter out empty lists\n",
    "        new_uris = new_uris[new_uris.apply(lambda x: len(x) > 0)].explode()\n",
    "        curies.extend(new_uris)\n",
    "        curies = list(set(curies))\n",
    "\n",
    "#curies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbcc25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a row for each\n",
    "for x in curies:\n",
    "    \n",
    "    # create a dictionary to hold information for this class\n",
    "    row = OrderedDict([])\n",
    "    for c in template.columns:\n",
    "        row[c] = \"\" # setting all as \"\" now to avoid awkward NaNs later\n",
    "    \n",
    "    # populate dictionary\n",
    "    \n",
    "    # annotation axioms\n",
    "    row['ID'] = x\n",
    "    if x.startswith('RO:'):\n",
    "        row['TYPE'] = 'owl:ObjectProperty'\n",
    "    else:\n",
    "        row['TYPE'] = 'owl:Class'\n",
    "    \n",
    "    # turn into dataframe and join to template\n",
    "    row_df = pd.DataFrame.from_dict([row])\n",
    "    template = pd.concat([template, row_df], ignore_index=True)\n",
    "\n",
    "#template.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e75200",
   "metadata": {},
   "outputs": [],
   "source": [
    "template.to_csv('template.tsv', sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04a1bf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
